# CÃ“DIGO COMPLETO E CORRIGIDO PARA: app/services/llm_service.py
# (Implementa o Agente de MÃºltiplas Etapas com Parallel Tool Calls)

import os
import json
import pytz
from datetime import datetime
from openai import OpenAI
from typing import List, Dict, Any, Optional, Iterator

class LLMService:
    def __init__(self, api_key: str = None, model: str = "gpt-4o-mini"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("Chave da API OpenAI nÃ£o fornecida")
        
        self.routing_model = "gpt-4o-mini"
        self.generation_model = model 
        
        self.client = OpenAI(api_key=self.api_key)
        self.token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

        # --- ARQUITETURA DE FERRAMENTAS ROBUSTA ---
        self.tool_ingest = {
            "type": "function",
            "function": {
                "name": "call_ingest_tool",
                "description": "Usado quando o usuÃ¡rio quer ingerir, re-ingerir ou indexar um repositÃ³rio.",
                "parameters": {
                    "type": "object",
                    "properties": {"repositorio": {"type": "string", "description": "O nome do repositÃ³rio no formato 'usuario/nome'."}},
                    "required": ["repositorio"],
                },
            },
        }
        
        self.tool_query = {
            "type": "function",
            "function": {
                "name": "call_query_tool",
                "description": "Usado para perguntas sobre um repositÃ³rio (RAG).",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "repositorio": {"type": "string", "description": "O nome do repositÃ³rio no formato 'usuario/nome'."},
                        "prompt_usuario": {"type": "string", "description": "A pergunta especÃ­fica do usuÃ¡rio."}
                    },
                    "required": ["repositorio", "prompt_usuario"],
                },
            },
        }

        self.tool_report = {
            "type": "function",
            "function": {
                "name": "call_report_tool",
                "description": "Usado para pedir um 'relatÃ³rio' ou 'grÃ¡fico' para DOWNLOAD IMEDIATO (salvar o arquivo no computador). Nunca use para email.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "repositorio": {"type": "string", "description": "O nome do repositÃ³rio no formato 'usuario/nome'."},
                        "prompt_usuario": {"type": "string", "description": "A instruÃ§Ã£o para o relatÃ³rio."}
                    },
                    "required": ["repositorio", "prompt_usuario"],
                },
            },
        }

        self.tool_schedule = {
            "type": "function",
            "function": {
                "name": "call_schedule_tool",
                "description": "Usado quando o usuÃ¡rio quer ENVIAR um relatÃ³rio por EMAIL (agora ou agendado). Use sempre que 'email', 'agendar' ou 'enviar' for mencionado.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "repositorio": {"type": "string", "description": "O nome do repositÃ³rio no formato 'usuario/nome'."},
                        "prompt_relatorio": {"type": "string", "description": "O que o relatÃ³rio deve conter."},
                        "frequencia": {"type": "string", "enum": ["once", "daily", "weekly", "monthly"], "description": "A frequÃªncia. Use 'once' para envio imediato."},
                        "hora": {"type": "string", "description": "A hora no formato HH:MM (24h)."},
                        "timezone": {"type": "string", "description": "O fuso horÃ¡rio (ex: 'America/Sao_Paulo')."},
                        "user_email": {"type": "string", "description": "O email do destinatÃ¡rio (ex: usuario@gmail.com)."}
                    },
                    "required": ["repositorio", "prompt_relatorio", "frequencia", "hora", "timezone"], 
                },
            },
        }
        
        self.tool_save_instruction = {
            "type": "function",
            "function": {
                "name": "call_save_instruction_tool",
                "description": "Usado para salvar uma instruÃ§Ã£o para futuros relatÃ³rios.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "repositorio": {"type": "string", "description": "O repositÃ³rio ao qual esta instruÃ§Ã£o se aplica."},
                        "instrucao": {"type": "string", "description": "A instruÃ§Ã£o especÃ­fica que o usuÃ¡rio quer salvar."}
                    },
                    "required": ["repositorio", "instrucao"],
                },
            },
        }

        self.tool_chat = {
            "type": "function",
            "function": {
                "name": "call_chat_tool",
                "description": "Usado para bate-papo casual, saudaÃ§Ãµes, ou respostas curtas. NENHUM argumento Ã© necessÃ¡rio se for um chat simples.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "prompt": {"type": "string", "description": "O texto do usuÃ¡rio."}
                    },
                },
            },
        }

        self.tools = [
            self.tool_ingest,
            self.tool_query,
            self.tool_report,
            self.tool_schedule,
            self.tool_save_instruction,
            self.tool_chat
        ]

        self.tool_map = {
            "call_ingest_tool": self.tool_ingest,
            "call_query_tool": self.tool_query,
            "call_report_tool": self.tool_report,
            "call_schedule_tool": self.tool_schedule,
            "call_save_instruction_tool": self.tool_save_instruction,
            "call_chat_tool": self.tool_chat
        }

    
    def get_intent(self, user_query: str) -> Dict[str, Any]:
        """
        Orquestra o roteamento. Agora retorna UMA ou MAIS ferramentas encadeadas.
        """
        if not self.client: raise Exception("LLMService nÃ£o inicializado")
        
        print(f"[LLMService] Iniciando Agente de Encadeamento para: '{user_query}'")
        
        # 1. Ajuste do prompt do sistema para o Agente
        system_prompt = f"""
VocÃª Ã© um Agente de Encadeamento de Tarefas que decomponhe um prompt de usuÃ¡rio em uma lista de etapas (chamadas de ferramenta) na ordem correta.

REGRAS CRÃTICAS DE ENCAMINHAMENTO:
1.  CHAME MÃšLTIPLAS FERRAMENTAS: Se o usuÃ¡rio pedir 'Ingira e depois gere relatÃ³rio', retorne [call_ingest_tool, call_report_tool/schedule] em ordem.
2.  EMAIL vs DOWNLOAD: Use APENAS call_schedule_tool para qualquer solicitaÃ§Ã£o que mencione 'email', 'agendar' ou 'enviar para mim'. Use APENAS call_report_tool para 'gerar relatÃ³rio' ou 'download'.
3.  INGESTÃƒO PRÃ‰VIA: Se o usuÃ¡rio pedir uma consulta, relatÃ³rio ou agendamento de um repositÃ³rio, inclua **call_ingest_tool** como o **PRIMEIRO** passo.
4.  VALIDE ARGUMENTOS: Se um argumento obrigatÃ³rio estiver faltando (como o nome do repo), vocÃª DEVE retornar uma resposta textual para CLARIFICAÃ‡ÃƒO. NUNCA tente inventar o nome do repositÃ³rio.
- Data/Hora: Hoje Ã© {datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%Y-%m-%d')}. O fuso horÃ¡rio padrÃ£o para agendamentos Ã© 'America/Sao_Paulo'.
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.routing_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query}
                ],
                tools=self.tools,
                tool_choice="auto" 
            )
            
            message = response.choices[0].message
            tool_calls = message.tool_calls

            if not tool_calls:
                # A IA decidiu que era um bate-papo simples (retornando apenas texto)
                chat_text = message.content or "Entendido."
                return {
                    "type": "simple_chat", 
                    "response_text": chat_text
                }

            # 2. Processa as chamadas de ferramenta (multi-step ou single-step)
            steps = []
            for call in tool_calls:
                try:
                    args = json.loads(call.function.arguments)
                    steps.append({
                        "intent": call.function.name,
                        "args": args
                    })
                except json.JSONDecodeError:
                    return {"type": "clarify", "response_text": "A IA falhou em formatar a requisiÃ§Ã£o. Por favor, reformule sua solicitaÃ§Ã£o."}
            
            # ValidaÃ§Ã£o: Se a IA tentou chamar uma ferramenta mas retornou campos vazios, Ã© falha na intenÃ§Ã£o.
            for step in steps:
                if step["intent"] != "call_chat_tool":
                    func_def = self.tool_map.get(step["intent"], {}).get("function", {})
                    required_params = func_def.get("parameters", {}).get("required", [])
                    
                    for param in required_params:
                        if not step["args"].get(param):
                            return {"type": "clarify", "response_text": f"O campo obrigatÃ³rio '{param}' estÃ¡ faltando. Por favor, forneÃ§a o valor."}

            print(f"[LLMService] IntenÃ§Ãµes detectadas: {len(steps)} etapas.")
            
            return {
                "type": "multi_step", 
                "steps": steps
            }

        except Exception as e:
            print(f"[LLMService] Erro no get_intent multi-step: {e}")
            return {"type": "clarify", "response_text": f"Erro interno ao processar: {e}"}

    
    def generate_response(self, query: str, context: List[Dict[str, Any]]) -> Dict[str, Any]:
        if not self.client: raise Exception("LLMService nÃ£o inicializado.")
        print("[LLMService] Iniciando resposta RAG (NÃƒO-Streaming)...")
        
        formatted_context = self._format_context(context)
        
        system_prompt = """
VocÃª Ã© um assistente de IA especialista em anÃ¡lise de repositÃ³rios GitHub.
Sua tarefa Ã© responder Ã  consulta do usuÃ¡rio com base estritamente no contexto fornecido (documentos de commits, issues e PRs).
Seja conciso e direto.
Se o contexto nÃ£o for suficiente, informe que nÃ£o encontrou informaÃ§Ãµes sobre aquele tÃ³pico especÃ­fico.
"""
        user_prompt = f"Contexto:\n{formatted_context}\n\nConsulta: \"{query}\"\n\nBaseado APENAS no contexto acima, responda Ã  consulta."

        try:
            response = self.client.chat.completions.create(
                model=self.generation_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1
            )
            
            usage = response.usage
            if usage:
                self.token_usage["prompt_tokens"] += usage.prompt_tokens
                self.token_usage["completion_tokens"] += usage.completion_tokens
                self.token_usage["total_tokens"] += usage.total_tokens

            response_text = response.choices[0].message.content
            return {"response": response_text, "usage": usage}

        except Exception as e:
            print(f"[LLMService] Erro durante o generate_response: {e}")
            return {"response": f"Erro ao gerar resposta: {e}", "usage": None}

    
    def generate_response_stream(self, query: str, context: List[Dict[str, Any]]) -> Iterator[str]:
        if not self.client: raise Exception("LLMService nÃ£o inicializado.")
        print("[LLMService] Iniciando resposta em STREAMING...")
        
        formatted_context = self._format_context(context)
        
        system_prompt = """
VocÃª Ã© um assistente de IA especialista em anÃ¡lise de repositÃ³rios GitHub.
Sua tarefa Ã© responder Ã  consulta do usuÃ¡rio com base estritamente no contexto fornecido (documentos de commits, issues e PRs).
Seja conciso e direto.
Se o contexto nÃ£o for suficiente, informe que nÃ£o encontrou informaÃ§Ãµes sobre aquele tÃ³pico especÃ­fico.
"""
        user_prompt = f"Contexto:\n{formatted_context}\n\nConsulta: \"{query}\"\n\nBaseado APENAS no contexto acima, responda Ã  consulta."

        try:
            stream = self.client.chat.completions.create(
                model=self.generation_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                stream=True,
                temperature=0.1
            )
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content
        except Exception as e:
            print(f"[LLMService] Erro durante o streaming: {e}")
            yield f"\n\n**Erro ao gerar resposta:** {e}"

    
    def generate_analytics_report(self, repo_name: str, user_prompt: str, raw_data: List[Dict[str, Any]]) -> str:
        context_json_string = json.dumps(raw_data)
        system_prompt = f"""
VocÃª Ã© um analista de dados...
REGRAS OBRIGATÃ“RIAS:
1.  **Formato:** O relatÃ³rio final DEVE ser um ÃšNICO objeto JSON.
2.  **Estrutura JSON:** `"analysis_markdown"` e `"chart_json"`...
... (exemplo de Chart.js) ...
"""
        final_user_prompt = f"""
Contexto do RepositÃ³rio: {repo_name}
Prompt do UsuÃ¡rio: "{user_prompt}"
Dados Brutos (JSON): {context_json_string}
---
Gere o relatÃ³rio em um Ãºnico objeto JSON...
"""
        try:
            response = self.client.chat.completions.create(
                model=self.generation_model, 
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": final_user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.3, max_tokens=4000
            )
            
            response_content = response.choices[0].message.content
            
            if not response_content:
                print("[LLMService] ERRO: OpenAI retornou None (provÃ¡vel filtro de conteÃºdo).")
                return json.dumps({
                    "analysis_markdown": "# Erro de GeraÃ§Ã£o\n\nA IA nÃ£o conseguiu gerar uma resposta. Isso pode ter sido causado por filtros de conteÃºdo ou uma falha na API.",
                    "chart_json": None
                })
            
            usage = response.usage
            self.token_usage["prompt_tokens"] += usage.prompt_tokens
            self.token_usage["completion_tokens"] += usage.completion_tokens
            self.token_usage["total_tokens"] += usage.total_tokens
            
            return response_content 

        except Exception as e:
            print(f"[LLMService] Erro ao gerar relatÃ³rio JSON: {e}")
            return json.dumps({
                "analysis_markdown": f"# Erro\n\nNÃ£o foi possÃ­vel gerar a anÃ¡lise: {e}",
                "chart_json": None
            })

    
    def generate_simple_response(self, prompt: str) -> str:
        print(f"[LLMService] Gerando resposta simples para: '{prompt}'")
        
        system_prompt = """
VocÃª Ã© um assistente de IA. Responda ao usuÃ¡rio de forma curta, casual e prestativa.
Se o usuÃ¡rio apenas disser 'ok', 'certo' ou 'correto', responda com 'ğŸ‘' ou 'Entendido.'.
Se o usuÃ¡rio disser 'obrigado', responda com 'De nada!' ou 'Estou aqui para ajudar!'.
"""
        try:
            response = self.client.chat.completions.create(
                model=self.routing_model, 
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=50
            )
            return response.choices[0].message.content
        
        except Exception as e:
            print(f"[LLMService] Erro ao gerar resposta simples: {e}")
            return "ğŸ‘" 

    
    def get_token_usage(self) -> Dict[str, int]:
        return self.token_usage

    def _format_context(self, context: List[Dict[str, Any]]) -> str:
        if not context:
            return "Nenhum contexto encontrado."
        
        formatted_text = ""
        for doc in context:
            meta = doc.get('metadata', {})
            tipo = meta.get('type', 'documento')
            conteudo = doc.get('text', '')
            
            formatted_text += f"--- Fonte (Tipo: {tipo}) ---\n"
            if 'url' in meta:
                formatted_text += f"URL: {meta.get('url')}\n"
            if 'autor' in meta:
                formatted_text += f"Autor: {meta.get('autor')}\n"
            if 'titulo' in meta:
                formatted_text += f"TÃ­tulo: {meta.get('titulo')}\n"
                
            formatted_text += f"ConteÃºdo: {conteudo}\n\n"
        
        return formatted_text

    def summarize_plan_for_confirmation(self, steps: List[Dict[str, Any]], user_email: str) -> str:
        """
        Gera uma pergunta de confirmaÃ§Ã£o humanizada baseada no plano de execuÃ§Ã£o.
        (Usa regras determinÃ­sticas para estabilidade e velocidade)
        """
        print(f"[LLMService] Gerando sumÃ¡rio de confirmaÃ§Ã£o para plano de {len(steps)} etapas (Deterministic)...")
        
        plan_summary_list = []
        for step in steps:
            intent = step['intent'].replace('call_', '').replace('_tool', '').capitalize()
            args = step['args']
            
            summary_line = f"* **{intent}:** "

            if intent == 'Ingest':
                summary_line += f"Ingerir o repositÃ³rio **{args.get('repositorio')}** para atualizar o RAG."
            elif intent == 'Query':
                summary_line += f"Consultar (RAG) o repositÃ³rio {args.get('repositorio')} com a pergunta: '{args.get('prompt_usuario', '')}'."
            elif intent == 'Report':
                summary_line += f"Gerar relatÃ³rio para **DOWNLOAD** do repositÃ³rio {args.get('repositorio')} (Prompt: '{args.get('prompt_usuario', '')}')."
            elif intent == 'Schedule':
                freq = args.get('frequencia')
                repo = args.get('repositorio')
                email = args.get('user_email') or user_email
                
                schedule_details = f"e enviar Imediatamente para o email **{email}**" if freq == 'once' else f"e agendar para **{freq}** Ã s {args.get('hora')} (fuso {args.get('timezone')})"
                
                summary_line += f"Preparar relatÃ³rio {schedule_details} (Repo: {repo})."
            elif intent == 'SaveInstruction':
                summary_line += f"Salvar a instruÃ§Ã£o para futuros relatÃ³rios do repositÃ³rio {args.get('repositorio')}."
            
            plan_summary_list.append(summary_line)

        plan_text = "\n".join(plan_summary_list)
        
        confirmation_message = f"""
**Ok, sÃ³ para confirmar o plano de execuÃ§Ã£o ({len(steps)} etapas):**
{plan_text}

As aÃ§Ãµes acima serÃ£o executadas em ordem sequencial (uma depende da anterior).
**Isso estÃ¡ correto?** (Responda 'sim' ou 'nÃ£o')
"""
        return confirmation_message

    def summarize_action_for_confirmation(self, intent_name: str, args: Dict[str, Any]) -> str:
        """
        [MANTIDA] - Gera a confirmaÃ§Ã£o para uma ÃšNICA aÃ§Ã£o de Agendamento Recorrente (Regra de NegÃ³cio Antiga).
        """
        print(f"[LLMService] Gerando sumÃ¡rio de confirmaÃ§Ã£o para agendamento recorrente: {intent_name}...")
        
        # NOTE: Esta funÃ§Ã£o agora lida apenas com o cenÃ¡rio de agendamento recorrente de um passo.
        # Caso 1: Agendamento Recorrente (a Ãºnica aÃ§Ã£o single-step que precisa de confirmaÃ§Ã£o)
        repo = args.get("repositorio")
        prompt = args.get("prompt_relatorio")
        freq = args.get("frequencia")
        hora = args.get("hora")
        tz = args.get("timezone")

        confirmation_text = f"""
Ok, sÃ³ para confirmar: Devo **agendar** o relatÃ³rio para o repositÃ³rio '{repo}' com o prompt: '{prompt[:50]}...',
com frequÃªncia **{freq}**, Ã s **{hora}** (fuso {tz}).

Isso estÃ¡ correto? (Responda 'sim' ou 'nÃ£o')
"""
        return confirmation_text

    def _format_requirements_data(self, requirements_data: List[Dict[str, Any]]) -> str:
        if not requirements_data:
            return "Nenhum dado de requisito fornecido."
        
        return json.dumps(requirements_data, indent=2, ensure_ascii=False)